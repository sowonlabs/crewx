# CrewX Configuration Example - Local AI Integration
# This file demonstrates how to integrate local AI tools (Ollama, Aider)
# Copy sections you need to your crewx.yaml file

# Plugin Providers
# Turn any CLI tool into an AI agent with simple YAML configuration
providers:
  # Ollama - Local LLM Integration
  # Run open-source models locally (Llama, Qwen, DeepSeek, etc.)
  - id: ollama
    type: plugin
    cli_command: ollama
    display_name: "Ollama Local LLM"
    description: "Local LLM via Ollama"
    default_model: "qwen3:8b"
    query_args:
      - "run"
      - "{model}"
    execute_args:
      - "run"
      - "{model}"
    prompt_in_args: false  # Ollama reads from stdin
    timeout:
      query: 120000   # 2 minutes
      execute: 300000 # 5 minutes

  # Ollama Remote - Connect to Ollama on another computer
  # Use this to connect to a remote Ollama server
  - id: ollama_remote
    type: plugin
    cli_command: ollama
    display_name: "Ollama Remote LLM"
    description: "Remote Ollama server on another computer"
    default_model: "qwen3:8b"
    query_args:
      - "run"
      - "{model}"
    execute_args:
      - "run"
      - "{model}"
    prompt_in_args: false
    env:
      OLLAMA_HOST: "http://192.168.1.100:11434"  # Replace with your remote server IP
    timeout:
      query: 180000   # 3 minutes (network latency)
      execute: 600000 # 10 minutes

  # Aider - AI Pair Programming
  # AI-powered code editing with local models
  - id: aider
    type: plugin
    cli_command: aider
    display_name: "Aider AI Coding Assistant"
    description: "AI pair programming tool with Ollama integration"
    default_model: "ollama/qwen3:8b"
    query_args:
      - "--yes-always"
      - "--no-browser"
      - "--no-auto-commits"
      - "--model"
      - "{model}"
      - "--message"
    execute_args:
      - "--yes-always"
      - "--no-browser"
      - "--no-auto-commits"
      - "--model"
      - "{model}"
      - "--message"
    prompt_in_args: true  # Aider takes message as argument
    timeout:
      query: 180000   # 3 minutes
      execute: 600000 # 10 minutes

  # Aider with Remote Ollama
  # Connect Aider to a remote Ollama server
  - id: aider_remote
    type: plugin
    cli_command: aider
    display_name: "Aider (Remote Ollama)"
    description: "Aider using remote Ollama server"
    default_model: "ollama/qwen3:8b"
    query_args:
      - "--yes-always"
      - "--no-browser"
      - "--no-auto-commits"
      - "--model"
      - "{model}"
      - "--message"
    execute_args:
      - "--yes-always"
      - "--no-browser"
      - "--no-auto-commits"
      - "--model"
      - "{model}"
      - "--message"
    prompt_in_args: true
    env:
      OLLAMA_API_BASE: "http://192.168.1.100:11434"  # Aider uses OLLAMA_API_BASE
    timeout:
      query: 300000   # 5 minutes
      execute: 900000 # 15 minutes

  # Crush - Z.AI Coding Assistant
  # Terminal-based AI assistant with GLM models
  - id: crush
    type: plugin
    cli_command: crush
    display_name: "Crush (Z.AI)"
    description: "Powerful terminal AI assistant with GLM-4.6 model"
    default_model: "glm-4.6"
    query_args:
      - "run"
      - "--quiet"  # Hide spinner
    execute_args:
      - "run"
      - "--quiet"  # Hide spinner (no --yolo: it's a global flag)
    prompt_in_args: true  # Crush takes message as argument
    timeout:
      query: 180000   # 3 minutes
      execute: 600000 # 10 minutes

# Agents Configuration
agents:
  # Ollama Qwen Agent (Local)
  - id: "ollama_qwen"
    name: "Qwen 3 (Local)"
    role: "assistant"
    team: "Local AI"
    description: "Local Qwen 3 model via Ollama"
    inline:
      type: "agent"
      provider: "plugin/ollama"
      model: "qwen3:8b"
      prompt: "You are a helpful AI assistant running locally via Ollama."

  # Ollama Qwen Agent (Remote)
  - id: "ollama_qwen_remote"
    name: "Qwen 3 (Remote)"
    role: "assistant"
    team: "Remote AI"
    description: "Remote Qwen 3 model via remote Ollama server"
    inline:
      type: "agent"
      provider: "plugin/ollama_remote"
      model: "qwen3:8b"
      prompt: "You are a helpful AI assistant running on a remote Ollama server."

  # Ollama DeepSeek Agent
  - id: "ollama_deepseek"
    name: "DeepSeek R1 (Local)"
    role: "assistant"
    team: "Local AI"
    description: "Local DeepSeek R1 model via Ollama"
    inline:
      type: "agent"
      provider: "plugin/ollama"
      model: "deepseek-r1:8b"
      prompt: "You are DeepSeek R1, a helpful AI assistant with reasoning capabilities."

  # Aider Coding Assistant (Local)
  - id: "aider_qwen"
    name: "Aider (Qwen 3)"
    role: "coding_assistant"
    team: "AI Pair Programming"
    description: "AI pair programming assistant using Aider with local Qwen 3 model"
    inline:
      type: "agent"
      provider: "plugin/aider"
      model: "ollama/qwen3:8b"

  # Aider Coding Assistant (Remote)
  - id: "aider_qwen_remote"
    name: "Aider Remote (Qwen 3)"
    role: "coding_assistant"
    team: "AI Pair Programming"
    description: "Aider using remote Ollama server"
    inline:
      type: "agent"
      provider: "plugin/aider_remote"
      model: "ollama/qwen3:8b"

  # Crush AI Assistant (Z.AI GLM-4.6)
  - id: "crush_glm"
    name: "Crush GLM-4.6"
    role: "coding_assistant"
    team: "Cloud AI"
    description: "Z.AI Crush terminal assistant with GLM-4.6 model"
    inline:
      type: "agent"
      provider: "plugin/crush"
      model: "glm-4.6"

# Usage Examples:
#
# 1. Chat with local Ollama model:
#    crewx query "@ollama_qwen explain quantum computing"
#
# 2. Chat with remote Ollama model:
#    crewx query "@ollama_qwen_remote explain quantum computing"
#
# 3. Generate code with local Aider:
#    crewx query "@aider_qwen create a fibonacci function in fib.js"
#
# 4. Generate code with remote Aider:
#    crewx query "@aider_qwen_remote refactor this code for better performance"
#
# 5. Use reasoning model:
#    crewx query "@ollama_deepseek solve this logic puzzle: ..."
#
# 6. Use Crush with Z.AI GLM-4.6:
#    crewx query "@crush_glm review this code for security issues"
#
# Prerequisites:
# - Ollama: brew install ollama (macOS) or visit https://ollama.ai
# - Pull models: ollama pull qwen3:8b && ollama pull deepseek-r1:8b
# - Aider: pip install aider-chat
# - Crush: npm install -g @charmland/crush (requires Z.AI API key)
#
# Remote Ollama Setup:
# On the remote server (192.168.1.100):
#   1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
#   2. Configure for remote access:
#      export OLLAMA_HOST=0.0.0.0:11434
#      ollama serve
#   3. Pull models on the server:
#      ollama pull qwen3:8b
#
# Benefits:
# - Local + Remote + Cloud options available
# - Ollama: 100% local, no API costs, privacy, works offline
# - Remote Ollama: Share GPU resources across multiple computers
# - Crush: Cloud-based Z.AI GLM-4.6, powerful coding assistant
# - Mix and match different AI services in one workflow
