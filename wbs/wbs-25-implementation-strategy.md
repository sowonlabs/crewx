# WBS-25 Advanced Features Implementation Strategy

> **Planning Date**: 2025-11-12
> **Status**: Planning Complete ‚úÖ
> **Ready for Execution**: Phase 1 - GREEN ‚úÖ

---

## Executive Summary

WBS-25 focuses on implementing **Streaming support** for API Providers. **Cost Tracking is NOT needed** (BYOA model - users pay directly to AI services).

**Key Findings**:
- ‚úÖ Mastra v0.24.0 has excellent streaming support (AI SDK v5)
- ‚úÖ No blockers identified for Phase 1
- ‚úÖ Implementation timeline reduced from 3 days to 1.5 days
- ‚ùå Cost tracking cancelled (BYOA model, no value)

---

## Current Analysis

### 1. Mastra Streaming Capabilities ‚úÖ EXCELLENT

**Good News**: Mastra has EXCELLENT streaming support:
- **v0.24.0** with AI SDK v5 integration (latest stable)
- `agent.stream()` method available with two output formats:
  - `format: 'mastra'` - Native Mastra streaming (default)
  - `format: 'aisdk'` - AI SDK v5 compatible streams
- **Nested streaming** support (agents calling agents)
- **Resumable streams** (automatic retry on connection loss)
- **Real-time tool calling** during streaming

**Key Insight**: Mastra v0.19.0+ made streaming the default experience. We're on v0.24.0 - fully mature streaming.

**Reference**:
- Mastra blog: "Nested streaming support" (July 2025)
- Mastra blog: "The next evolution of Mastra streaming" (September 2025)
- AI SDK v5 announcement (improved agent orchestration)

### 2. Current MastraAPIProvider Implementation

**File**: `packages/sdk/src/core/providers/MastraAPIProvider.ts`

**Current State**:
- ‚úÖ Uses `agent.generate()` for non-streaming queries
- ‚ùå No streaming implementation yet
- ‚úÖ Tool calling support via MastraToolAdapter
- ‚úÖ 7 providers working (OpenAI, Anthropic, Google, Bedrock, LiteLLM, Ollama, SowonAI)

**Current Architecture**:
```typescript
export class MastraAPIProvider implements AIProvider {
  async query(prompt: string, options: AIQueryOptions = {}): Promise<AIResponse> {
    const agent = new Agent({
      name: this.config.provider,
      model: this.modelInstance,
      instructions: prompt,
      tools: mastraTools,
    });

    const result = await agent.generate(prompt); // ‚Üê Non-streaming
    return this.convertResponse(result, taskId);
  }
}
```

**Architecture**: Clean separation - easy to add streaming without breaking existing code

### 3. CLI Display Requirements

**File**: `packages/cli/src/cli/query.handler.ts`

**Current Flow**:
1. Parse query with agent mentions (`@agent message`)
2. Call `crewXTool.queryAgent()` ‚Üí Returns `AIResponse`
3. Display `result.response` (full text at once)

**For Streaming**: Need progressive output like CLI providers

**Reference Pattern** (Claude CLI Provider):
```typescript
// packages/sdk/src/core/providers/claude.provider.ts
// Supports --output-format stream-json for progressive output
```

---

## Phase-by-Phase Implementation Plan

### Phase 1: Streaming Support (1 day) ‚¨úÔ∏è

**Goal**: Add streaming to MastraAPIProvider with CLI progressive output

#### Task 1.1: Add streaming method to MastraAPIProvider (3 hours)

**New Method Signature**:
```typescript
/**
 * Query agent with streaming response
 *
 * Returns async iterable for progressive output
 */
async queryStream(
  prompt: string,
  options: AIQueryOptions = {}
): AsyncIterable<AIStreamChunk>
```

**Implementation Approach**:
```typescript
async *queryStream(prompt: string, options: AIQueryOptions = {}): AsyncIterable<AIStreamChunk> {
  const agent = new Agent({
    name: this.config.provider,
    model: this.modelInstance,
    instructions: prompt,
    tools: mastraTools,
  });

  // Use Mastra's stream() method with 'mastra' format
  const stream = await agent.stream([
    { role: 'user', content: prompt }
  ], {
    format: 'mastra' // Native Mastra streaming
  });

  // Process stream events
  for await (const chunk of stream) {
    if (chunk.type === 'text') {
      yield { type: 'text', content: chunk.text };
    } else if (chunk.type === 'tool-call') {
      yield {
        type: 'tool_call',
        content: `[Tool: ${chunk.toolName}]`,
        toolCall: { name: chunk.toolName, input: chunk.toolInput }
      };
    } else if (chunk.type === 'done') {
      yield { type: 'done', content: '' };
    } else if (chunk.type === 'error') {
      yield { type: 'error', content: '', error: chunk.error };
    }
  }
}
```

**Files to Modify**:
- `packages/sdk/src/core/providers/MastraAPIProvider.ts`

**Testing**:
- Streaming with OpenAI (`api/openai`)
- Streaming with Anthropic (`api/anthropic`)
- Tool calling during streaming
- Error handling

---

#### Task 1.2: Extend AIProvider interface (1 hour)

**Add Optional Streaming Method**:
```typescript
// packages/sdk/src/core/providers/ai-provider.interface.ts

export type AIStreamChunk = {
  type: 'text' | 'tool_call' | 'done' | 'error';
  content: string;
  toolCall?: {
    name: string;
    input: any;
  };
  error?: string;
};

export interface AIProvider {
  readonly name: string;
  isAvailable(): Promise<boolean>;
  query(prompt: string, options?: AIQueryOptions): Promise<AIResponse>;
  execute(prompt: string, options?: AIQueryOptions): Promise<AIResponse>;
  getToolPath(): Promise<string | null>;

  // NEW: Optional streaming support
  queryStream?(
    prompt: string,
    options?: AIQueryOptions
  ): AsyncIterable<AIStreamChunk>;
}
```

**Backward Compatibility**:
- `queryStream` is OPTIONAL (indicated by `?`)
- CLI providers (Claude/Gemini/Copilot) don't need to implement this
- Only API providers will implement streaming

**Files to Modify**:
- `packages/sdk/src/core/providers/ai-provider.interface.ts`

---

#### Task 1.3: CLI streaming output (2 hours)

**Implementation Strategy**:
```typescript
// packages/cli/src/cli/query.handler.ts

// Detect if provider supports streaming
if (args.stream && provider.queryStream) {
  // Use streaming
  let fullResponse = '';

  for await (const chunk of provider.queryStream(query, options)) {
    if (chunk.type === 'text') {
      process.stdout.write(chunk.content); // Progressive output
      fullResponse += chunk.content;
    } else if (chunk.type === 'tool_call') {
      process.stdout.write(`\n[Tool: ${chunk.toolCall.name}]\n`);
    } else if (chunk.type === 'error') {
      console.error(`\nError: ${chunk.error}`);
      break;
    }
  }

  // Save full response to conversation history
  if (conversationProvider && threadId) {
    await conversationProvider.addMessage(threadId, 'crewx', fullResponse, true);
  }
} else {
  // Use standard non-streaming query
  const result = await provider.query(query, options);
  console.log(result.content);
}
```

**CLI Output Format**:
```
üîé Querying agent: @my_api_agent
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Here is the response streaming in real-time...
[Tool: search_web]
Results: ...
Continuing response...

‚úÖ Done
```

**Files to Modify**:
- `packages/cli/src/cli/query.handler.ts`

**Maintain Compatibility**:
- `--raw` mode: output only streaming text (no decorations)
- Non-streaming mode: works as before
- Conversation history: save full accumulated response

---

#### Task 1.4: Add `--stream` flag (1 hour)

**CLI Option Definition**:
```typescript
// packages/cli/src/cli-options.ts

export interface CliOptions {
  // ... existing options

  /**
   * Enable streaming output (progressive display)
   * Only works with API providers that support streaming
   * @default false
   */
  stream?: boolean;
}
```

**Usage Examples**:
```bash
# Non-streaming (default)
crewx q "@my_api_agent hello"

# Streaming enabled
crewx q "@my_api_agent hello" --stream

# Raw streaming (no decorations)
crewx q "@my_api_agent hello" --stream --raw
```

**Files to Modify**:
- `packages/cli/src/cli-options.ts`
- `packages/cli/src/cli/query.handler.ts` (parse `--stream` flag)

---

#### Phase 1 Deliverables

**Code Changes**:
- ‚úÖ `queryStream()` method in `MastraAPIProvider`
- ‚úÖ `AIStreamChunk` type definition
- ‚úÖ CLI progressive output logic
- ‚úÖ `--stream` flag support

**Tests**:
- ‚úÖ `packages/sdk/tests/integration/streaming.test.ts` (5+ tests):
  - Streaming with OpenAI
  - Streaming with Anthropic
  - Tool calling during streaming
  - Error handling (network interruption)
  - Backward compatibility (no --stream flag)

**Documentation**:
- ‚úÖ Update `docs/api-provider-guide.md` (streaming section)
- ‚úÖ Add streaming example to `examples/api-agent-streaming/`

---

### Phase 2: Cost Tracking Analysis ‚ùå CANCELLED

**Decision**: **NOT IMPLEMENTING COST TRACKING**

#### Rationale

**1. BYOA Model (Bring Your Own API Key)**:
- CrewX is a framework, not a service provider
- Users pay directly to OpenAI/Anthropic/Google/etc
- CrewX SDK doesn't handle billing or payment
- Token usage is visible in each provider's dashboard

**2. Implementation Complexity vs Value**:
- Need per-model pricing database (100+ models)
  - OpenAI: 50+ models (GPT-3.5, GPT-4, GPT-4 Turbo, etc.)
  - Anthropic: Claude 3 family (Opus, Sonnet, Haiku)
  - Google: Gemini Pro, Gemini Ultra, PaLM 2
  - Each has different prices per 1K tokens
- Prices change frequently (OpenAI price drops every quarter)
- Different regions/tiers have different prices
  - OpenAI Tier 1/2/3/4 pricing
  - Volume discounts
  - Regional variations
- OpenAI/Anthropic provide usage APIs already
  - OpenAI: `/v1/dashboard/usage`
  - Anthropic: Usage dashboard
  - Google: Cloud Console billing

**3. Alternative Approaches (Better for Users)**:
- **Users check usage in provider dashboards**:
  - OpenAI Dashboard: https://platform.openai.com/usage
  - Anthropic Console: https://console.anthropic.com/usage
  - Google Cloud: https://console.cloud.google.com/billing
- **For enterprise: Use LiteLLM gateway**:
  - LiteLLM has built-in cost tracking
  - Supports 100+ models with pricing database
  - `api/litellm` provider already supported
- **For power users: Export raw token counts** (future enhancement):
  - `crewx export-usage --format json`
  - Users calculate costs themselves with custom pricing

#### Comparison: Implementing vs Not Implementing

| Factor | If We Implement | If We Don't |
|--------|----------------|-------------|
| Maintenance | High (update prices monthly) | None |
| Accuracy | Medium (can't track volume discounts) | Perfect (provider dashboard is source of truth) |
| User Value | Low (users already have dashboards) | No loss (dashboards are better) |
| Implementation Time | 1 day + ongoing maintenance | 0 days |
| Enterprise Use | Prefer LiteLLM gateway (centralized tracking) | Same |

#### Final Decision

**‚úÖ APPROVED: Do NOT implement cost tracking**

**Status**: Analysis complete - **No implementation needed**

---

### Phase 3: Advanced Error Handling (0.5 day) ‚¨úÔ∏è

**Goal**: Robust error handling for streaming and API failures

#### Task 3.1: Stream error handling (2 hours)

**Error Scenarios**:
1. Network interruption during streaming
2. Provider API timeout
3. Malformed chunks
4. Tool execution errors

**Implementation**:
```typescript
async *queryStream(prompt: string, options: AIQueryOptions = {}): AsyncIterable<AIStreamChunk> {
  try {
    const stream = await agent.stream([...]);

    for await (const chunk of stream) {
      yield { type: 'text', content: chunk.text };
    }

    yield { type: 'done', content: '' };
  } catch (error: any) {
    // Emit error chunk
    yield {
      type: 'error',
      content: '',
      error: error.message || 'Stream error'
    };

    // Graceful degradation: fall back to generate()
    logger.warn('Streaming failed, falling back to generate()');
    const result = await this.query(prompt, options); // Use non-streaming
    yield { type: 'text', content: result.content };
    yield { type: 'done', content: '' };
  }
}
```

**Files to Modify**:
- `packages/sdk/src/core/providers/MastraAPIProvider.ts`

---

#### Task 3.2: Basic retry logic (1-2 hours)

**Retry Strategy**:
- **Retry on**: 5xx errors, network errors, timeouts
- **Do NOT retry on**: 4xx errors (auth, rate limit, invalid request)
- **Max retries**: 2 (configurable)
- **Backoff**: Exponential (1s, 2s, 4s)

**Configuration**:
```typescript
// packages/sdk/src/types/api-provider.types.ts
export interface APIProviderConfig {
  // ... existing fields

  /**
   * Maximum number of retries on transient errors
   * @default 2
   */
  maxRetries?: number;

  /**
   * Base delay for exponential backoff (ms)
   * @default 1000
   */
  retryDelayMs?: number;
}
```

**Implementation**:
```typescript
private async executeWithRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 2
): Promise<T> {
  let lastError: Error | undefined;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error: any) {
      lastError = error;

      // Don't retry on 4xx errors
      if (error.status >= 400 && error.status < 500) {
        throw error;
      }

      // Don't retry on last attempt
      if (attempt === maxRetries) {
        throw error;
      }

      // Exponential backoff
      const delay = this.config.retryDelayMs || 1000;
      await new Promise(resolve => setTimeout(resolve, delay * Math.pow(2, attempt)));
    }
  }

  throw lastError;
}

async query(prompt: string, options: AIQueryOptions = {}): Promise<AIResponse> {
  return this.executeWithRetry(async () => {
    // Existing query logic
    const agent = new Agent({...});
    const result = await agent.generate(prompt);
    return this.convertResponse(result, taskId);
  }, this.config.maxRetries);
}
```

**Files to Modify**:
- `packages/sdk/src/core/providers/MastraAPIProvider.ts`
- `packages/sdk/src/types/api-provider.types.ts`

---

#### Task 3.3: Timeout improvements (1 hour)

**Streaming-Aware Timeout**:
- Problem: Total timeout doesn't work for streaming (can take minutes)
- Solution: Per-chunk timeout (default: 30s)

**Implementation**:
```typescript
async *queryStream(prompt: string, options: AIQueryOptions = {}): AsyncIterable<AIStreamChunk> {
  const chunkTimeout = options.timeout || 30000; // 30s per chunk
  const stream = await agent.stream([...]);

  for await (const chunk of stream) {
    // Timeout per chunk, not total time
    const chunkPromise = Promise.race([
      chunk,
      new Promise((_, reject) =>
        setTimeout(() => reject(new Error('Chunk timeout')), chunkTimeout)
      )
    ]);

    const result = await chunkPromise;
    yield { type: 'text', content: result.text };
  }
}
```

**Configuration**:
```typescript
// packages/sdk/src/types/api-provider.types.ts
export interface APIProviderConfig {
  // ... existing fields

  /**
   * Timeout per streaming chunk (ms)
   * Only applies to streaming mode
   * @default 30000
   */
  streamTimeout?: number;
}
```

**Files to Modify**:
- `packages/sdk/src/core/providers/MastraAPIProvider.ts`
- `packages/sdk/src/types/api-provider.types.ts`

---

#### Phase 3 Deliverables

**Code Changes**:
- ‚úÖ Graceful error handling in `queryStream()`
- ‚úÖ Basic retry logic (max 2 retries)
- ‚úÖ Per-chunk timeout for streaming
- ‚úÖ Configuration options in `APIProviderConfig`

**Tests**:
- ‚úÖ `packages/sdk/tests/integration/error-handling.test.ts` (3+ tests):
  - Network interruption during streaming
  - Retry on 5xx errors
  - No retry on 401/403/429
  - Timeout scenario

**Documentation**:
- ‚úÖ Update error handling section in `docs/api-provider-guide.md`

---

## Blockers & Dependencies

### ‚úÖ No Blockers Identified

**Green Light for Phase 1 Execution**:
1. ‚úÖ Mastra v0.24.0 has stable streaming API
   - Production-ready (used by Gatsby team)
   - AI SDK v5 integration complete
   - Nested streaming support
2. ‚úÖ MastraAPIProvider architecture is clean
   - Easy to add `queryStream()` without breaking existing code
   - Tool calling already abstracted via `MastraToolAdapter`
3. ‚úÖ CLI already handles progressive output
   - Claude CLI provider uses `--output-format stream-json`
   - Pattern exists, just adapt for API providers
4. ‚úÖ No breaking changes to existing API
   - `queryStream()` is optional method
   - `--stream` flag is opt-in
   - Existing queries work unchanged

### Dependencies

**Phase 1 Dependencies** (all satisfied ‚úÖ):
- ‚úÖ WBS-24 completed (CLI integration) - DONE 2025-11-12
- ‚úÖ MastraAPIProvider working - DONE 2025-11-12
- ‚úÖ Tool calling functional - DONE 2025-11-12
- ‚úÖ TypeScript 5.x with async iterators - Already using

**Phase 3 Dependencies**:
- Phase 1 (streaming must work before error handling)

---

## Implementation Timeline

**Total Duration**: 1.5 days (vs 3 days in original WBS-25)

| Phase | Duration | Start After | Priority | Status |
|-------|----------|-------------|----------|--------|
| Phase 1: Streaming | 1 day | WBS-24 ‚úÖ | P0 | ‚¨úÔ∏è Ready |
| Phase 2: Cost Tracking | 0 day | ‚ùå CANCELLED | - | ‚úÖ Analysed |
| Phase 3: Error Handling | 0.5 day | Phase 1 | P1 | ‚¨úÔ∏è Waiting |

**Why Faster than Original Plan?**
- Original: 1.5 days streaming + 1 day cost tracking + 0.5 day error handling = 3 days
- Updated: 1 day streaming + 0 day cost tracking + 0.5 day error handling = 1.5 days
- **Time Saved**: 1.5 days (50% reduction)

**Reasons**:
1. Cost tracking removed (0 implementation)
2. Mastra handles heavy lifting (AI SDK v5)
3. CLI pattern already exists (Claude provider reference)

---

## Risk Assessment

### Low Risk ‚úÖ

**1. Streaming Implementation**:
- **Risk**: Mastra API instability
  - **Mitigation**: v0.24.0 is stable, used in production by Gatsby
  - **Evidence**: Mastra changelog shows mature streaming since v0.19.0
- **Risk**: CLI output formatting issues
  - **Mitigation**: Copy pattern from `claude.provider.ts` (--output-format stream-json)

**2. Backward Compatibility**:
- **Risk**: Breaking existing queries
  - **Mitigation**: `--stream` flag is opt-in (default: false)
  - **Mitigation**: `queryStream()` is optional interface method

**3. Tool Calling + Streaming**:
- **Risk**: Tools break during streaming
  - **Mitigation**: Mastra supports real-time tool calling natively
  - **Evidence**: Mastra docs show `toolCallChunk` event type

### Medium Risk ‚ö†Ô∏è

**1. Phase 3 Error Handling**:
- **Risk**: Retry logic complexity (infinite loops, thundering herd)
  - **Mitigation**: Keep simple (max 2 retries only)
  - **Mitigation**: Exponential backoff prevents thundering herd
  - **Mitigation**: No retry on 4xx errors (prevents infinite auth loops)

**2. Streaming Performance**:
- **Risk**: High latency on slow connections
  - **Mitigation**: Per-chunk timeout (30s default)
  - **Mitigation**: Graceful fallback to `generate()` on stream failure

### Negligible Risk ‚úÖ

**1. Cost Tracking Removal**:
- **Risk**: Users complain about missing cost tracking
  - **Mitigation**: BYOA model makes this unnecessary
  - **Mitigation**: Provider dashboards are source of truth
  - **Impact**: None (users already use provider dashboards)

---

## Success Criteria

### Phase 1 (Streaming) - Must Pass ‚úÖ

- [ ] **Streaming Output**: `crewx q "@my_api_agent hello" --stream` shows progressive output
- [ ] **Tool Calling**: Tools execute during streaming with progress indication
- [ ] **Conversation History**: Full response saved to thread after streaming completes
- [ ] **Backward Compatibility**: `crewx q "@my_api_agent hello"` (no --stream) works unchanged
- [ ] **Raw Mode**: `crewx q "@my_api_agent hello" --stream --raw` outputs clean text
- [ ] **Multiple Providers**: Streaming works with OpenAI, Anthropic, Google
- [ ] **Tests**: 5+ integration tests passing

### Phase 3 (Error Handling) - Must Pass ‚úÖ

- [ ] **Network Errors**: Auto-retry on 5xx errors (max 2x)
- [ ] **Graceful Degradation**: Falls back to `generate()` if streaming fails
- [ ] **Clear Errors**: User-friendly error messages
- [ ] **No Infinite Loops**: No retry on 401/403/429
- [ ] **Tests**: 3+ error scenario tests passing

### Overall WBS-25 Success ‚úÖ

- [ ] **Production Ready**: Streaming works reliably in CLI
- [ ] **Documentation**: User guide updated with streaming examples
- [ ] **Examples**: `examples/api-agent-streaming/` demonstrates usage
- [ ] **No Regressions**: All existing tests still passing

---

## Post-Planning Actions

### Immediate Next Steps (After Approval)

**1. Create Phase 1 Branch**:
```bash
git checkout develop
git pull origin develop
git checkout -b feature/wbs-25-streaming
```

**2. Implement Phase 1 (Day 1)**:
- Morning (3h): Task 1.1 - Add `queryStream()` to MastraAPIProvider
- Late Morning (1h): Task 1.2 - Extend AIProvider interface
- Afternoon (2h): Task 1.3 - CLI streaming output
- Late Afternoon (1h): Task 1.4 - Add `--stream` flag

**3. Testing (Day 1 Evening)**:
- Write 5+ integration tests
- Manual testing with OpenAI/Anthropic
- Test tool calling during streaming

**4. Phase 1 PR Review (Day 2 Morning)**:
- Submit PR to `develop`
- Address review comments
- Merge after approval

**5. Implement Phase 3 (Day 2 Afternoon - 0.5 day)**:
- Task 3.1: Stream error handling (2h)
- Task 3.2: Basic retry logic (1-2h)
- Task 3.3: Timeout improvements (1h)
- Write 3+ error tests

**6. Phase 3 PR Review (Day 2 Evening)**:
- Submit PR to `develop`
- Merge after approval

**7. Documentation & Examples (Day 3 Morning)**:
- Update `docs/api-provider-guide.md`
- Create `examples/api-agent-streaming/`
- Update WBS-25 status in `wbs.md`

**8. Final Testing (Day 3 Afternoon)**:
- End-to-end regression testing
- Performance testing (measure latency improvement)
- User acceptance testing

### Status Update to Team

**After Phase 1 Complete**:
```bash
crewx q "@crewx_claude_dev Phase 1 complete. Streaming working with OpenAI/Anthropic. Ready for Phase 3?"
```

**After Phase 3 Complete**:
```bash
crewx q "@crewx_claude_dev WBS-25 complete. All tests passing. Ready for production?"
```

---

## Questions for Approval

**Before Phase 1 Execution**:
1. ‚úÖ Approved: Cost tracking removal (BYOA model)
2. ‚úÖ Approved: Streaming as opt-in (`--stream` flag)
3. ‚ùì Question: Should streaming be default in future releases? (after stability proven)
4. ‚ùì Question: Priority for Phase 3? (Can ship Phase 1 alone if needed)

**Technical Decisions Needed**:
1. ‚úÖ Use Mastra native format (`format: 'mastra'`) not `format: 'aisdk'`
2. ‚úÖ Per-chunk timeout (30s) not total timeout
3. ‚úÖ Max 2 retries on transient errors
4. ‚ùì Should we expose `maxRetries` in YAML config or keep as TypeScript-only?

---

## Appendix: Technical References

### Mastra Streaming API

**Agent.stream() Signature**:
```typescript
stream<OUTPUT extends OutputSchema = undefined, FORMAT extends 'mastra' | 'aisdk' | undefined = undefined>(
  messages: MessageListInput,
  streamOptions?: AgentExecutionOptions<OUTPUT, FORMAT>
): Promise<FORMAT extends 'aisdk' ? AISDKV5OutputStream<OUTPUT> : MastraModelOutput<OUTPUT>>
```

**Mastra Stream Chunk Types**:
```typescript
type MastraStreamChunk =
  | { type: 'text'; text: string }
  | { type: 'tool-call'; toolName: string; toolInput: any }
  | { type: 'tool-result'; toolName: string; result: any }
  | { type: 'done' }
  | { type: 'error'; error: string }
```

**Example Usage** (from Mastra docs):
```typescript
const stream = await agent.stream([
  { role: 'user', content: 'Hello' }
], { format: 'mastra' });

for await (const chunk of stream) {
  if (chunk.type === 'text') {
    console.log(chunk.text);
  }
}
```

### CLI Provider Streaming Reference

**Claude Provider** (`packages/sdk/src/core/providers/claude.provider.ts`):
```typescript
// Uses --output-format stream-json
// Parses JSONL (JSON Lines) output
// Extracts final result from stream
private parseJSONLOutput(output: string): any {
  const lines = output.trim().split('\n');
  for (const line of lines.reverse()) {
    if (line.startsWith('{')) {
      return JSON.parse(line);
    }
  }
}
```

---

## Conclusion

**WBS-25 is READY for execution**:
- ‚úÖ No blockers identified
- ‚úÖ Clear technical approach
- ‚úÖ 50% time reduction (cost tracking removed)
- ‚úÖ Low risk implementation
- ‚úÖ Backward compatible

**Recommendation**: **Approve Phase 1 execution immediately**

**Expected Delivery**: 1.5 days (vs 3 days original estimate)

---

**Planning completed by**: @crewx_claude_dev
**Date**: 2025-11-12
**Status**: ‚úÖ Ready for approval and execution
